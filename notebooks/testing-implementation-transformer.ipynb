{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2025-03-25T12:58:52.343328900Z",
     "start_time": "2025-03-25T12:58:41.892412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from architectures.scaled_dot_product import scaled_dot_product_attention\n",
    "from architectures.TransformerLayer import TransformerLayer\n",
    "from architectures.MHA import MultiheadAttention"
   ],
   "id": "69f4c1970afd5b72",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tessting Transformer\n",
    "\n",
    "In this section, I'll test the Transformer step by step.\n",
    "\n",
    "Starting with scaled dot-product attention and multi-head attention, I'll then progress to Transformer layers and ultimately the Transformer encoder-decoder model."
   ],
   "id": "620bf232bbeafcc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Scaled Dot-Product Attention\n",
    "\n",
    "In this section, I'll implement Scaled Dot-Product Attention. The input types and shapes are:\n",
    "\n",
    "- `q`: `Tensor[n, tgt_len, d_head]`\n",
    "- `k`: `Tensor[n, src_len, d_head]`\n",
    "- `v`: `Tensor[n, src_len, d_head]`\n",
    "- `key_padding_mask`: `Tensor[n, src_len]`\n",
    "- `causal`: `bool`\n",
    "\n",
    "`n` represents the total number of attention operations calculated in parallel.\n",
    "\n",
    "In multi-head attention, it's usually the product of the batch size and the number of attention heads. For each of the `n` operations, we will compute attention scores\n",
    "\n",
    "$$s_{i,j} = \\mathbf{q}_i^T \\mathbf{k}_j / \\sqrt{d_\\text{head}}$$\n",
    "\n",
    "Then we Apply softmax to the attention score, then use it as weights to linearly combine values in `v`:\n",
    "\n",
    "$$a_{i,j} = \\dfrac{\\exp(s_{i,j})}{\\sum_k \\exp(s_{i, k})}$$\n",
    "\n",
    "$$\\mathrm{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V})_i = \\sum_j a_{i,j} \\mathbf{v}_j$$\n",
    "\n",
    "We consider two essential details: *padding* and *causal masking*.\n",
    "\n",
    "- *padding*: The Transformer input usually contains sentences of varying lengths, padded into a tensor. Attention should ignore pad tokens, so they don't impact the results. `key_padding_mask` is a byte tensor set to **1** in pad token positions. If `key_padding_mask` is `None`, there's no padding in the input.\n",
    "\n",
    "- *causal masking*: Autoregressive generation in the decoder uses causal attention masks, meaning position $i$ can only attend to position $j$ if $i \\ge j$. If `causal` is set to true, apply causal attention masking. The provided `future_mask` may be useful."
   ],
   "id": "61033e1966aceb62"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T12:56:40.694900Z",
     "start_time": "2025-03-25T12:56:39.719308Z"
    }
   },
   "cell_type": "code",
   "source": "future_mask = torch.triu(torch.zeros([1024, 1024]).fill_(float(\"-inf\")), 1)",
   "id": "902a9e874ab1cff1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m future_mask \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtriu(torch\u001B[38;5;241m.\u001B[39mzeros([\u001B[38;5;241m1024\u001B[39m, \u001B[38;5;241m1024\u001B[39m])\u001B[38;5;241m.\u001B[39mfill_(\u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m-inf\u001B[39m\u001B[38;5;124m\"\u001B[39m)), \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Testing Implementation of scaled dot product",
   "id": "ef1a625ba3154938"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T12:47:11.884553Z",
     "start_time": "2025-03-25T12:47:11.685051Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_scaled_dot_product_attention():\n",
    "    q1 = torch.tensor([[1, 0, 0], [0.5, 0.5, 0]]).view(1, 2, 3).float()\n",
    "    k1 = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]]).view(1, 3, 3).float()\n",
    "    v1 = torch.tensor([[3, 0, 0], [0, 5, 0], [0, 0, 7]]).view(1, 3, 3).float()\n",
    "    o1 = scaled_dot_product_attention(q1, k1, v1)\n",
    "    assert list(o1.shape) == [1, 2, 3]\n",
    "    assert torch.allclose(\n",
    "        o1.view(-1)[:5],\n",
    "        torch.tensor([1.413249135017395, 1.3222922086715698, 1.8512091636657715, 1.0912044048309326, 1.818674087524414]).float(),\n",
    "        rtol=1e-3\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(100)\n",
    "    q2 = torch.randn(3, 5, 7).float()\n",
    "    k2 = torch.randn(3, 11, 7).float()\n",
    "    v2 = torch.randn(3, 11, 7).float()\n",
    "    o2 = scaled_dot_product_attention(q2, k2, v2)\n",
    "    assert list(o2.shape) == [3, 5, 7]\n",
    "    assert torch.allclose(\n",
    "        o2.view(-1)[6: 11],\n",
    "        torch.tensor([-0.40304261445999146, -0.2931785583496094, 0.20563912391662598, 0.08719107508659363, 0.08274038136005402]).float(),\n",
    "        rtol=1e-3\n",
    "    )\n",
    "\n",
    "    key_padding_mask = torch.tensor([[0, 0, 1]]).byte()\n",
    "    o4 = scaled_dot_product_attention(q1, k1, v1, key_padding_mask=key_padding_mask)\n",
    "    assert list(o4.shape) == [1, 2, 3]\n",
    "    assert torch.allclose(\n",
    "        o4.view(-1)[:5],\n",
    "        torch.tensor([1.921372413635254, 1.7977124452590942, 0.0, 1.5, 2.5]),\n",
    "        rtol=1e-3\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(210)\n",
    "    q5 = torch.randn(2, 4, 3).float()\n",
    "    k5 = torch.randn(2, 4, 3).float()\n",
    "    v5 = torch.randn(2, 4, 3).float()\n",
    "    o5 = scaled_dot_product_attention(q5, k5, v5, causal=True)\n",
    "    assert list(o5.shape) == [2, 4, 3]\n",
    "    assert torch.allclose(\n",
    "        o5.view(-1)[2: 7],\n",
    "        torch.tensor([0.9079901576042175, -0.573272705078125, -1.1765587329864502, 0.7771514058113098, -0.3235766291618347]),\n",
    "        rtol=1e-3\n",
    "    )\n",
    "\n",
    "\n",
    "test_scaled_dot_product_attention()"
   ],
   "id": "fc9887df554c8411",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 55\u001B[0m\n\u001B[0;32m     47\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(o5\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m==\u001B[39m [\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m3\u001B[39m]\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mallclose(\n\u001B[0;32m     49\u001B[0m         o5\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m2\u001B[39m: \u001B[38;5;241m7\u001B[39m],\n\u001B[0;32m     50\u001B[0m         torch\u001B[38;5;241m.\u001B[39mtensor([\u001B[38;5;241m0.9079901576042175\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.573272705078125\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1.1765587329864502\u001B[39m, \u001B[38;5;241m0.7771514058113098\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m0.3235766291618347\u001B[39m]),\n\u001B[0;32m     51\u001B[0m         rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m\n\u001B[0;32m     52\u001B[0m     )\n\u001B[1;32m---> 55\u001B[0m test_scaled_dot_product_attention()\n",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m, in \u001B[0;36mtest_scaled_dot_product_attention\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtest_scaled_dot_product_attention\u001B[39m():\n\u001B[1;32m----> 2\u001B[0m     q1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m], [\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0\u001B[39m]])\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m      3\u001B[0m     k1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([[\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m]])\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[0;32m      4\u001B[0m     v1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor([[\u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m5\u001B[39m, \u001B[38;5;241m0\u001B[39m], [\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m7\u001B[39m]])\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m3\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Multi-head Attention\n",
    "\n",
    "In this section, I'll implement multi-head attention.\n",
    "\n",
    "The input to this layer has types and shapes:\n",
    "\n",
    "- `q`: `Tensor[bsz, tgt_len, d_model]`\n",
    "- `k`: `Tensor[bsz, src_len, d_model]`\n",
    "- `v`: `Tensor[bsz, src_len, d_model]`\n",
    "- `key_padding_mask`: `Tensor[bsz, src_len]`\n",
    "- `causal`: `bool`\n",
    "\n",
    "A multi-head attention layer has four linear projection layers (including biases): `q_proj`, `k_proj`, `v_proj`, and `o_proj`. `q_proj`, `k_proj`, and `v_proj` project `q`, `k`, and `v` respectively into `n_heads` `d_head` vectors. The shapes of the projected query, key, and value will be `[bsz, tgt_len, n_heads, d_head]`, `[bsz, src_len, n_heads, d_head]`, and `[bsz, src_len, n_heads, d_head]` respectively.\n",
    "\n",
    "In the provided code below, instead of creating `n_heads` projection matrices of `d_model -> d_head` for the query/key/value, we use a single projection matrix of `d_model -> d_model`. This means the first `d_head` channels correspond to the first attention head, and channels from `d_head + 1` to `2 * d_head` correspond to the second attention head, and so on. The same rule applies to the input channels of `o_proj`.\n",
    "\n",
    "Next, we  rearrange the projected query, key, and value tensors appropriately and feed them into the previously implemented `scaled_dot_product_attention` function.\n",
    "\n",
    "The output of `scaled_dot_product_attention` should then be projected by o_proj to produce the final output. The output shape should be `[bsz, tgt_len, d_model]`."
   ],
   "id": "4b393bdca879b11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Testing Implementation",
   "id": "fe25bf5b3be58146"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def test_multihead_attention():\n",
    "    torch.manual_seed(350)\n",
    "    mha0 = nn.MultiheadAttention(embed_dim=128, num_heads=4, batch_first=True)\n",
    "    nn.init.normal_(mha0.in_proj_weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(mha0.in_proj_bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(mha0.out_proj.weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(mha0.out_proj.bias, mean=0.0, std=0.05)\n",
    "    mha1 = MultiheadAttention(128, 4)\n",
    "    mha1.q_proj.weight.data.copy_(mha0.in_proj_weight.data[:128, :])\n",
    "    mha1.q_proj.bias.data.copy_(mha0.in_proj_bias.data[:128])\n",
    "    mha1.k_proj.weight.data.copy_(mha0.in_proj_weight.data[128:256, :])\n",
    "    mha1.k_proj.bias.data.copy_(mha0.in_proj_bias.data[128:256])\n",
    "    mha1.v_proj.weight.data.copy_(mha0.in_proj_weight.data[256:, :])\n",
    "    mha1.v_proj.bias.data.copy_(mha0.in_proj_bias.data[256:])\n",
    "    mha1.o_proj.weight.data.copy_(mha0.out_proj.weight.data)\n",
    "    mha1.o_proj.bias.data.copy_(mha0.out_proj.bias.data)\n",
    "\n",
    "    torch.manual_seed(400)\n",
    "    q1 = torch.randn(4, 6, 128).float()\n",
    "    k1 = torch.randn(4, 6, 128).float()\n",
    "    v1 = torch.randn(4, 6, 128).float()\n",
    "    assert torch.allclose(\n",
    "        mha0(q1, k1, v1)[0].contiguous(),\n",
    "        mha1(q1, k1, v1).contiguous(),\n",
    "        rtol=1e-3\n",
    "    )\n",
    "\n",
    "    torch.manual_seed(600)\n",
    "    q3 = torch.randn(4, 6, 128).float()\n",
    "    k3 = torch.randn(4, 6, 128).float()\n",
    "    v3 = torch.randn(4, 6, 128).float()\n",
    "    key_padding_mask = torch.tensor([\n",
    "        [0, 0, 1, 1, 1, 1],\n",
    "        [0, 0, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 1]\n",
    "    ]).byte()\n",
    "\n",
    "    o30 = mha0(\n",
    "        q3, k3, v3,\n",
    "        key_padding_mask=key_padding_mask.to(torch.bool),\n",
    "        attn_mask=future_mask[:6, :6]\n",
    "    )[0].contiguous()\n",
    "\n",
    "    o31 = mha1(q3, k3, v3, key_padding_mask=key_padding_mask, causal=True).contiguous()\n",
    "\n",
    "    assert torch.allclose(o30[0, :2], o31[0, :2], rtol=1e-3)\n",
    "    assert torch.allclose(o30[0, :4], o31[0, :4], rtol=1e-3)\n",
    "    assert torch.allclose(o30[0, :6], o31[0, :6], rtol=1e-3)\n",
    "    assert torch.allclose(o30[0, :5], o31[0, :5], rtol=1e-3)\n",
    "\n",
    "test_multihead_attention()"
   ],
   "id": "a1b3190a84a4da26"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Transformer Layers\n",
    "\n",
    "In this section, I'll **implement Transformer Encoder/Decoder layers** according to Figure 1 of \"Attention is All You Need\".\n",
    "\n",
    "I also apply residual dropout (Section 5.4 of the paper) and attention dropout.\n",
    "\n",
    "The `is_decoder` flag determines if this Transformer layer is an encoder or a decoder.\n",
    "\n",
    "If it's an encoder, the input types and shapes will be:\n",
    "\n",
    "- `x`: `Tensor[bsz, src_len, d_model]`\n",
    "- `padding_mask`: `Tensor[bsz, src_len]`\n",
    "\n",
    "If it's a decoder, the input types and shapes will be:\n",
    "\n",
    "- `x`: `Tensor[bsz, tgt_len, d_model]`\n",
    "- `padding_mask`: `Tensor[bsz, tgt_len]`\n",
    "- `encoder_out`: `Tensor[bsz, src_len, d_model]`\n",
    "- `encoder_padding_mask`: `Tensor[bsz, src_len]`\n",
    "\n",
    "The output is a tensor of the same shape as `x`."
   ],
   "id": "944f6954b6001df4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Testing Implementation",
   "id": "4a82c23087febba7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T12:49:26.157958Z",
     "start_time": "2025-03-25T12:49:25.835616Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_transformer_layer():\n",
    "    torch.manual_seed(750)\n",
    "    enc_layer0 = nn.TransformerEncoderLayer(128, 4, dim_feedforward=512, dropout=0.0, batch_first=True)\n",
    "    nn.init.normal_(enc_layer0.self_attn.in_proj_weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(enc_layer0.self_attn.in_proj_bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(enc_layer0.self_attn.out_proj.weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(enc_layer0.self_attn.out_proj.bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(enc_layer0.linear1.weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(enc_layer0.linear1.bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(enc_layer0.linear2.weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(enc_layer0.linear2.bias, mean=0.0, std=0.05)\n",
    "    enc_layer1 = TransformerLayer(False, 128, 4, 512, 0.0)\n",
    "    enc_layer1.self_attn.q_proj.weight.data.copy_(enc_layer0.self_attn.in_proj_weight.data[:128, :])\n",
    "    enc_layer1.self_attn.q_proj.bias.data.copy_(enc_layer0.self_attn.in_proj_bias.data[:128])\n",
    "    enc_layer1.self_attn.k_proj.weight.data.copy_(enc_layer0.self_attn.in_proj_weight.data[128:256, :])\n",
    "    enc_layer1.self_attn.k_proj.bias.data.copy_(enc_layer0.self_attn.in_proj_bias.data[128:256])\n",
    "    enc_layer1.self_attn.v_proj.weight.data.copy_(enc_layer0.self_attn.in_proj_weight.data[256:, :])\n",
    "    enc_layer1.self_attn.v_proj.bias.data.copy_(enc_layer0.self_attn.in_proj_bias.data[256:])\n",
    "    enc_layer1.self_attn.o_proj.weight.data.copy_(enc_layer0.self_attn.out_proj.weight.data)\n",
    "    enc_layer1.self_attn.o_proj.bias.data.copy_(enc_layer0.self_attn.out_proj.bias.data)\n",
    "    enc_layer1.fc1.weight.data.copy_(enc_layer0.linear1.weight.data)\n",
    "    enc_layer1.fc1.bias.data.copy_(enc_layer0.linear1.bias.data)\n",
    "    enc_layer1.fc2.weight.data.copy_(enc_layer0.linear2.weight.data)\n",
    "    enc_layer1.fc2.bias.data.copy_(enc_layer0.linear2.bias.data)\n",
    "\n",
    "    torch.manual_seed(800)\n",
    "    x = torch.randn(4, 5, 128).float()\n",
    "    x_mask = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 1, 1], [0, 0, 0, 0, 1], [0, 0, 1, 1, 1]]).byte()\n",
    "    y10 = enc_layer0(x, src_key_padding_mask=x_mask.to(torch.bool)).contiguous()\n",
    "    y11 = enc_layer1(x, x_mask).contiguous()\n",
    "    assert torch.allclose(y10[0], y11[0], rtol=1e-3)\n",
    "    assert torch.allclose(y10[1, :3], y11[1, :3], rtol=1e-3)\n",
    "    assert torch.allclose(y10[2, :4], y11[2, :4], rtol=1e-3)\n",
    "    assert torch.allclose(y10[3, :2], y11[3, :2], rtol=1e-3)\n",
    "\n",
    "    torch.manual_seed(950)\n",
    "    dec_layer0 = nn.TransformerDecoderLayer(128, 4, dim_feedforward=512, dropout=0.0, batch_first=True)\n",
    "    nn.init.normal_(dec_layer0.self_attn.in_proj_weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.self_attn.in_proj_bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.self_attn.out_proj.weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.self_attn.out_proj.bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.multihead_attn.in_proj_weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.multihead_attn.in_proj_bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.multihead_attn.out_proj.weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.multihead_attn.out_proj.bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.linear1.weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.linear1.bias, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.linear2.weight, mean=0.0, std=0.05)\n",
    "    nn.init.normal_(dec_layer0.linear2.bias, mean=0.0, std=0.05)\n",
    "    dec_layer1 = TransformerLayer(True, 128, 4, 512, 0.0)\n",
    "    dec_layer1.self_attn.q_proj.weight.data.copy_(dec_layer0.self_attn.in_proj_weight.data[:128, :])\n",
    "    dec_layer1.self_attn.q_proj.bias.data.copy_(dec_layer0.self_attn.in_proj_bias.data[:128])\n",
    "    dec_layer1.self_attn.k_proj.weight.data.copy_(dec_layer0.self_attn.in_proj_weight.data[128:256, :])\n",
    "    dec_layer1.self_attn.k_proj.bias.data.copy_(dec_layer0.self_attn.in_proj_bias.data[128:256])\n",
    "    dec_layer1.self_attn.v_proj.weight.data.copy_(dec_layer0.self_attn.in_proj_weight.data[256:, :])\n",
    "    dec_layer1.self_attn.v_proj.bias.data.copy_(dec_layer0.self_attn.in_proj_bias.data[256:])\n",
    "    dec_layer1.self_attn.o_proj.weight.data.copy_(dec_layer0.self_attn.out_proj.weight.data)\n",
    "    dec_layer1.self_attn.o_proj.bias.data.copy_(dec_layer0.self_attn.out_proj.bias.data)\n",
    "    dec_layer1.cross_attn.q_proj.weight.data.copy_(dec_layer0.multihead_attn.in_proj_weight.data[:128, :])\n",
    "    dec_layer1.cross_attn.q_proj.bias.data.copy_(dec_layer0.multihead_attn.in_proj_bias.data[:128])\n",
    "    dec_layer1.cross_attn.k_proj.weight.data.copy_(dec_layer0.multihead_attn.in_proj_weight.data[128:256, :])\n",
    "    dec_layer1.cross_attn.k_proj.bias.data.copy_(dec_layer0.multihead_attn.in_proj_bias.data[128:256])\n",
    "    dec_layer1.cross_attn.v_proj.weight.data.copy_(dec_layer0.multihead_attn.in_proj_weight.data[256:, :])\n",
    "    dec_layer1.cross_attn.v_proj.bias.data.copy_(dec_layer0.multihead_attn.in_proj_bias.data[256:])\n",
    "    dec_layer1.cross_attn.o_proj.weight.data.copy_(dec_layer0.multihead_attn.out_proj.weight.data)\n",
    "    dec_layer1.cross_attn.o_proj.bias.data.copy_(dec_layer0.multihead_attn.out_proj.bias.data)\n",
    "    dec_layer1.fc1.weight.data.copy_(dec_layer0.linear1.weight.data)\n",
    "    dec_layer1.fc1.bias.data.copy_(dec_layer0.linear1.bias.data)\n",
    "    dec_layer1.fc2.weight.data.copy_(dec_layer0.linear2.weight.data)\n",
    "    dec_layer1.fc2.bias.data.copy_(dec_layer0.linear2.bias.data)\n",
    "\n",
    "    torch.manual_seed(1000)\n",
    "    x = torch.randn(4, 5, 128).float()\n",
    "    e = torch.randn(4, 3, 128).float()\n",
    "    x_mask = torch.tensor([[0, 0, 0, 0, 0], [0, 0, 0, 1, 1], [0, 0, 0, 0, 1], [0, 0, 1, 1, 1]]).byte()\n",
    "    e_mask = torch.tensor([[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 1]]).byte()\n",
    "    y30 = dec_layer0(x, e, tgt_mask=future_mask[:5, :5], tgt_key_padding_mask=x_mask.to(torch.bool), memory_key_padding_mask=e_mask.to(torch.bool)).contiguous()\n",
    "    y31 = dec_layer1(x, x_mask, e, e_mask).contiguous()\n",
    "    assert torch.allclose(y30[0], y31[0], rtol=1e-3)\n",
    "    assert torch.allclose(y30[1, :3], y31[1, :3], rtol=1e-3)\n",
    "    assert torch.allclose(y30[2, :4], y31[2, :4], rtol=1e-3)\n",
    "    assert torch.allclose(y30[3, :2], y31[3, :2], rtol=1e-3)\n",
    "\n",
    "\n",
    "test_transformer_layer()"
   ],
   "id": "9917d701a30f3d1",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 85\u001B[0m\n\u001B[0;32m     81\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mallclose(y30[\u001B[38;5;241m2\u001B[39m, :\u001B[38;5;241m4\u001B[39m], y31[\u001B[38;5;241m2\u001B[39m, :\u001B[38;5;241m4\u001B[39m], rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[0;32m     82\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mallclose(y30[\u001B[38;5;241m3\u001B[39m, :\u001B[38;5;241m2\u001B[39m], y31[\u001B[38;5;241m3\u001B[39m, :\u001B[38;5;241m2\u001B[39m], rtol\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-3\u001B[39m)\n\u001B[1;32m---> 85\u001B[0m test_transformer_layer()\n",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m, in \u001B[0;36mtest_transformer_layer\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtest_transformer_layer\u001B[39m():\n\u001B[1;32m----> 2\u001B[0m     torch\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m750\u001B[39m)\n\u001B[0;32m      3\u001B[0m     enc_layer0 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mTransformerEncoderLayer(\u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m4\u001B[39m, dim_feedforward\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m512\u001B[39m, dropout\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m     nn\u001B[38;5;241m.\u001B[39minit\u001B[38;5;241m.\u001B[39mnormal_(enc_layer0\u001B[38;5;241m.\u001B[39mself_attn\u001B[38;5;241m.\u001B[39min_proj_weight, mean\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, std\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.05\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'torch' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Putting them together: Transformer\n",
    "\n",
    "In this section, I will implement a Transformer encoder-decoder model for sequence-to-sequence tasks using the building blocks we've already created: scaled dot-product attention, multi-head attention, and Transformer encoder/decoder layers.\n",
    "\n",
    "Model Overview:\n",
    "\n",
    "- Encoder: `n_layers` layers\n",
    "- Decoder: `n_layers` layers\n",
    "- Learned positional embeddings instead of sinusoidal positional encodings (as in \"Attention is All You Need\")\n",
    "- Shared embedding matrices to reduced number of parameters and to improve training stability:\n",
    "  - Encoder input embeddings\n",
    "  - Decoder input embeddings\n",
    "  - Decoder output layer weights (a linear classifier over n_words vocabulary, whose weight matrix happens to have the same shape as word embeddings, so their weights can be shared (section 3.4 of the paper))\n",
    "- Layer normalization after the embedding layers\n",
    "- Shared positional embedding matrix and normalization layer for both encoder and decoder\n",
    "- Decoder's first input token: `[EOS]`\n",
    "- Handling of pad tokens in labels (-100). Huggingface `transformers` pads labels with padding index -100 instead of `pad_id`. So we do processing as follows:\n",
    "  - Replace -100 in decoder input\n",
    "  - Ignore -100 in labels when calculating loss\n",
    "\n",
    "We Implement the `make_positions` method to generate input for the positional embedding layer and take care of the pad tokens by repeating the last position\n",
    "\n",
    "Given a `padding_mask`:\n",
    "\n",
    "```\n",
    "[[0, 0, 0, 0, 1],\n",
    " [0, 0, 0, 1, 1]]\n",
    "```\n",
    "We would return:\n",
    "\n",
    "```\n",
    "[[0, 1, 2, 3, 3],\n",
    " [0, 1, 2, 2, 2]]\n",
    "```"
   ],
   "id": "2f7346cdb19a920d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Testing implementation:\n",
    "\n",
    "To test the final transformer implementation, we will train it on the x-sum dataset, we expect a validation loss of around 4.5. \n",
    "\n",
    "Run the `main` function using a bach command inside Colab or Kaggle."
   ],
   "id": "df3dec18eea653fd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
